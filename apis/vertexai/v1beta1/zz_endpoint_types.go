// SPDX-FileCopyrightText: 2024 The Crossplane Authors <https://crossplane.io>
//
// SPDX-License-Identifier: Apache-2.0

// Code generated by upjet. DO NOT EDIT.

package v1beta1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type AutomaticResourcesInitParameters struct {
}

type AutomaticResourcesObservation struct {

	// (Output)
	// The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
	MaxReplicaCount *float64 `json:"maxReplicaCount,omitempty" tf:"max_replica_count,omitempty"`

	// (Output)
	// The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
	MinReplicaCount *float64 `json:"minReplicaCount,omitempty" tf:"min_replica_count,omitempty"`
}

type AutomaticResourcesParameters struct {
}

type AutoscalingMetricSpecsInitParameters struct {
}

type AutoscalingMetricSpecsObservation struct {

	// (Output)
	// The resource metric name. Supported metrics: * For Online Prediction: * aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle * aiplatform.googleapis.com/prediction/online/cpu/utilization
	MetricName *string `json:"metricName,omitempty" tf:"metric_name,omitempty"`

	// (Output)
	// The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
	Target *float64 `json:"target,omitempty" tf:"target,omitempty"`
}

type AutoscalingMetricSpecsParameters struct {
}

type BigqueryDestinationInitParameters struct {

	// BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: - BigQuery path. For example: bq://projectId or bq://projectId.bqDatasetId or bq://projectId.bqDatasetId.bqTableId.
	OutputURI *string `json:"outputUri,omitempty" tf:"output_uri,omitempty"`
}

type BigqueryDestinationObservation struct {

	// BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: - BigQuery path. For example: bq://projectId or bq://projectId.bqDatasetId or bq://projectId.bqDatasetId.bqTableId.
	OutputURI *string `json:"outputUri,omitempty" tf:"output_uri,omitempty"`
}

type BigqueryDestinationParameters struct {

	// BigQuery URI to a project or table, up to 2000 characters long. When only the project is specified, the Dataset and Table is created. When the full table reference is specified, the Dataset must exist and table must not exist. Accepted forms: - BigQuery path. For example: bq://projectId or bq://projectId.bqDatasetId or bq://projectId.bqDatasetId.bqTableId.
	// +kubebuilder:validation:Optional
	OutputURI *string `json:"outputUri,omitempty" tf:"output_uri,omitempty"`
}

type DedicatedResourcesInitParameters struct {
}

type DedicatedResourcesObservation struct {

	// (Output)
	// The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to aiplatform.googleapis.com/prediction/online/cpu/utilization and autoscaling_metric_specs.target to 80.
	// Structure is documented below.
	AutoscalingMetricSpecs []AutoscalingMetricSpecsObservation `json:"autoscalingMetricSpecs,omitempty" tf:"autoscaling_metric_specs,omitempty"`

	// (Output)
	// The specification of a single machine used by the prediction.
	// Structure is documented below.
	MachineSpec []MachineSpecObservation `json:"machineSpec,omitempty" tf:"machine_spec,omitempty"`

	// (Output)
	// The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
	MaxReplicaCount *float64 `json:"maxReplicaCount,omitempty" tf:"max_replica_count,omitempty"`

	// (Output)
	// The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
	MinReplicaCount *float64 `json:"minReplicaCount,omitempty" tf:"min_replica_count,omitempty"`
}

type DedicatedResourcesParameters struct {
}

type DeployedModelsInitParameters struct {
}

type DeployedModelsObservation struct {

	// (Output)
	// A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
	// Structure is documented below.
	AutomaticResources []AutomaticResourcesObservation `json:"automaticResources,omitempty" tf:"automatic_resources,omitempty"`

	// (Output)
	// Output only. Timestamp when the DeployedModel was created.
	CreateTime *string `json:"createTime,omitempty" tf:"create_time,omitempty"`

	// (Output)
	// A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
	// Structure is documented below.
	DedicatedResources []DedicatedResourcesObservation `json:"dedicatedResources,omitempty" tf:"dedicated_resources,omitempty"`

	// (Output)
	// The display name of the DeployedModel. If not provided upon creation, the Model's display_name is used.
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// (Output)
	// These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
	EnableAccessLogging *bool `json:"enableAccessLogging,omitempty" tf:"enable_access_logging,omitempty"`

	// (Output)
	// If true, the container of the DeployedModel instances will send stderr and stdout streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
	EnableContainerLogging *bool `json:"enableContainerLogging,omitempty" tf:"enable_container_logging,omitempty"`

	// (Output)
	// The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// (Output)
	// The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
	Model *string `json:"model,omitempty" tf:"model,omitempty"`

	// (Output)
	// Output only. The version ID of the model that is deployed.
	ModelVersionID *string `json:"modelVersionId,omitempty" tf:"model_version_id,omitempty"`

	// (Output)
	// Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
	// Structure is documented below.
	PrivateEndpoints []PrivateEndpointsObservation `json:"privateEndpoints,omitempty" tf:"private_endpoints,omitempty"`

	// (Output)
	// The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the iam.serviceAccounts.actAs permission on this service account.
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account,omitempty"`

	// (Output)
	// The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
	SharedResources *string `json:"sharedResources,omitempty" tf:"shared_resources,omitempty"`
}

type DeployedModelsParameters struct {
}

type EndpointEncryptionSpecInitParameters struct {

	// Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`
}

type EndpointEncryptionSpecObservation struct {

	// Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
	KMSKeyName *string `json:"kmsKeyName,omitempty" tf:"kms_key_name,omitempty"`
}

type EndpointEncryptionSpecParameters struct {

	// Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
	// +kubebuilder:validation:Optional
	KMSKeyName *string `json:"kmsKeyName" tf:"kms_key_name,omitempty"`
}

type EndpointInitParameters struct {

	// If true, the endpoint will be exposed through a dedicated DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS will be isolated from other users' traffic and will have better performance and reliability. Note: Once you enabled dedicated endpoint, you won't be able to send request to the shared DNS {region}-aiplatform.googleapis.com. The limitation will be removed soon.
	DedicatedEndpointEnabled *bool `json:"dedicatedEndpointEnabled,omitempty" tf:"dedicated_endpoint_enabled,omitempty"`

	// The description of the Endpoint.
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// Customer-managed encryption key spec for an Endpoint. If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.
	// Structure is documented below.
	EncryptionSpec *EndpointEncryptionSpecInitParameters `json:"encryptionSpec,omitempty" tf:"encryption_spec,omitempty"`

	// The labels with user-defined metadata to organize your Endpoints. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field effective_labels for all of the labels present on the resource.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// The full name of the Google Compute Engine network to which the Endpoint should be peered. Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network. Only one of the fields, network or enable_private_service_connect, can be set. Format: projects/{project}/global/networks/{network}. Where {project} is a project number, as in 12345, and {network} is network name. Only one of the fields, network or privateServiceConnectConfig, can be set.
	Network *string `json:"network,omitempty" tf:"network,omitempty"`

	// Configures the request-response logging for online prediction.
	// Structure is documented below.
	PredictRequestResponseLoggingConfig *PredictRequestResponseLoggingConfigInitParameters `json:"predictRequestResponseLoggingConfig,omitempty" tf:"predict_request_response_logging_config,omitempty"`

	// Configuration for private service connect. network and privateServiceConnectConfig are mutually exclusive.
	// Structure is documented below.
	PrivateServiceConnectConfig *PrivateServiceConnectConfigInitParameters `json:"privateServiceConnectConfig,omitempty" tf:"private_service_connect_config,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// The region for the resource
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	// A map from a DeployedModel's id to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel.
	// If a DeployedModel's id is not listed in this map, then it receives no traffic.
	// The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at a moment. See
	// the deployModel example and
	// documentation for more information.
	// ~> Note: To set the map to empty, set "{}", apply, and then remove the field from your config.
	TrafficSplit *string `json:"trafficSplit,omitempty" tf:"traffic_split,omitempty"`
}

type EndpointObservation struct {

	// Output only. Timestamp when this Endpoint was created.
	CreateTime *string `json:"createTime,omitempty" tf:"create_time,omitempty"`

	// Output only. DNS of the dedicated endpoint. Will only be populated if dedicatedEndpointEnabled is true. Format: https://{endpointId}.{region}-{projectNumber}.prediction.vertexai.goog.
	DedicatedEndpointDNS *string `json:"dedicatedEndpointDns,omitempty" tf:"dedicated_endpoint_dns,omitempty"`

	// If true, the endpoint will be exposed through a dedicated DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS will be isolated from other users' traffic and will have better performance and reliability. Note: Once you enabled dedicated endpoint, you won't be able to send request to the shared DNS {region}-aiplatform.googleapis.com. The limitation will be removed soon.
	DedicatedEndpointEnabled *bool `json:"dedicatedEndpointEnabled,omitempty" tf:"dedicated_endpoint_enabled,omitempty"`

	// Output only. The models deployed in this Endpoint. To add or remove DeployedModels use EndpointService.DeployModel and EndpointService.UndeployModel respectively. Models can also be deployed and undeployed using the Cloud Console.
	// Structure is documented below.
	DeployedModels []DeployedModelsObservation `json:"deployedModels,omitempty" tf:"deployed_models,omitempty"`

	// The description of the Endpoint.
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// +mapType=granular
	EffectiveLabels map[string]*string `json:"effectiveLabels,omitempty" tf:"effective_labels,omitempty"`

	// Customer-managed encryption key spec for an Endpoint. If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.
	// Structure is documented below.
	EncryptionSpec *EndpointEncryptionSpecObservation `json:"encryptionSpec,omitempty" tf:"encryption_spec,omitempty"`

	// Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
	Etag *string `json:"etag,omitempty" tf:"etag,omitempty"`

	// an identifier for the resource with format projects/{{project}}/locations/{{location}}/endpoints/{{name}}
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// The labels with user-defined metadata to organize your Endpoints. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field effective_labels for all of the labels present on the resource.
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// The location for the resource
	Location *string `json:"location,omitempty" tf:"location,omitempty"`

	// Output only. Resource name of the Model Monitoring job associated with this Endpoint if monitoring is enabled by CreateModelDeploymentMonitoringJob. Format: projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}
	ModelDeploymentMonitoringJob *string `json:"modelDeploymentMonitoringJob,omitempty" tf:"model_deployment_monitoring_job,omitempty"`

	// The full name of the Google Compute Engine network to which the Endpoint should be peered. Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network. Only one of the fields, network or enable_private_service_connect, can be set. Format: projects/{project}/global/networks/{network}. Where {project} is a project number, as in 12345, and {network} is network name. Only one of the fields, network or privateServiceConnectConfig, can be set.
	Network *string `json:"network,omitempty" tf:"network,omitempty"`

	// Configures the request-response logging for online prediction.
	// Structure is documented below.
	PredictRequestResponseLoggingConfig *PredictRequestResponseLoggingConfigObservation `json:"predictRequestResponseLoggingConfig,omitempty" tf:"predict_request_response_logging_config,omitempty"`

	// Configuration for private service connect. network and privateServiceConnectConfig are mutually exclusive.
	// Structure is documented below.
	PrivateServiceConnectConfig *PrivateServiceConnectConfigObservation `json:"privateServiceConnectConfig,omitempty" tf:"private_service_connect_config,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// The region for the resource
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	// The combination of labels configured directly on the resource
	// and default labels configured on the provider.
	// +mapType=granular
	TerraformLabels map[string]*string `json:"terraformLabels,omitempty" tf:"terraform_labels,omitempty"`

	// A map from a DeployedModel's id to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel.
	// If a DeployedModel's id is not listed in this map, then it receives no traffic.
	// The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at a moment. See
	// the deployModel example and
	// documentation for more information.
	// ~> Note: To set the map to empty, set "{}", apply, and then remove the field from your config.
	TrafficSplit *string `json:"trafficSplit,omitempty" tf:"traffic_split,omitempty"`

	// Output only. Timestamp when this Endpoint was last updated.
	UpdateTime *string `json:"updateTime,omitempty" tf:"update_time,omitempty"`
}

type EndpointParameters struct {

	// If true, the endpoint will be exposed through a dedicated DNS [Endpoint.dedicated_endpoint_dns]. Your request to the dedicated DNS will be isolated from other users' traffic and will have better performance and reliability. Note: Once you enabled dedicated endpoint, you won't be able to send request to the shared DNS {region}-aiplatform.googleapis.com. The limitation will be removed soon.
	// +kubebuilder:validation:Optional
	DedicatedEndpointEnabled *bool `json:"dedicatedEndpointEnabled,omitempty" tf:"dedicated_endpoint_enabled,omitempty"`

	// The description of the Endpoint.
	// +kubebuilder:validation:Optional
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
	// +kubebuilder:validation:Optional
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// Customer-managed encryption key spec for an Endpoint. If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	EncryptionSpec *EndpointEncryptionSpecParameters `json:"encryptionSpec,omitempty" tf:"encryption_spec,omitempty"`

	// The labels with user-defined metadata to organize your Endpoints. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	// Note: This field is non-authoritative, and will only manage the labels present in your configuration.
	// Please refer to the field effective_labels for all of the labels present on the resource.
	// +kubebuilder:validation:Optional
	// +mapType=granular
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// The location for the resource
	// +kubebuilder:validation:Required
	Location *string `json:"location" tf:"location,omitempty"`

	// The full name of the Google Compute Engine network to which the Endpoint should be peered. Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network. Only one of the fields, network or enable_private_service_connect, can be set. Format: projects/{project}/global/networks/{network}. Where {project} is a project number, as in 12345, and {network} is network name. Only one of the fields, network or privateServiceConnectConfig, can be set.
	// +kubebuilder:validation:Optional
	Network *string `json:"network,omitempty" tf:"network,omitempty"`

	// Configures the request-response logging for online prediction.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	PredictRequestResponseLoggingConfig *PredictRequestResponseLoggingConfigParameters `json:"predictRequestResponseLoggingConfig,omitempty" tf:"predict_request_response_logging_config,omitempty"`

	// Configuration for private service connect. network and privateServiceConnectConfig are mutually exclusive.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	PrivateServiceConnectConfig *PrivateServiceConnectConfigParameters `json:"privateServiceConnectConfig,omitempty" tf:"private_service_connect_config,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	Project *string `json:"project,omitempty" tf:"project,omitempty"`

	// The region for the resource
	// +kubebuilder:validation:Optional
	Region *string `json:"region,omitempty" tf:"region,omitempty"`

	// A map from a DeployedModel's id to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel.
	// If a DeployedModel's id is not listed in this map, then it receives no traffic.
	// The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at a moment. See
	// the deployModel example and
	// documentation for more information.
	// ~> Note: To set the map to empty, set "{}", apply, and then remove the field from your config.
	// +kubebuilder:validation:Optional
	TrafficSplit *string `json:"trafficSplit,omitempty" tf:"traffic_split,omitempty"`
}

type MachineSpecInitParameters struct {
}

type MachineSpecObservation struct {

	// (Output)
	// The number of accelerators to attach to the machine.
	AcceleratorCount *float64 `json:"acceleratorCount,omitempty" tf:"accelerator_count,omitempty"`

	// (Output)
	// The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values here.
	AcceleratorType *string `json:"acceleratorType,omitempty" tf:"accelerator_type,omitempty"`

	// (Output)
	// The type of the machine. See the list of machine types supported for prediction See the list of machine types supported for custom training. For DeployedModel this field is optional, and the default value is n1-standard-2. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
	MachineType *string `json:"machineType,omitempty" tf:"machine_type,omitempty"`
}

type MachineSpecParameters struct {
}

type PredictRequestResponseLoggingConfigInitParameters struct {

	// BigQuery table for logging. If only given a project, a new dataset will be created with name logging_<endpoint-display-name>_<endpoint-id> where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name request_response_logging
	// Structure is documented below.
	BigqueryDestination *BigqueryDestinationInitParameters `json:"bigqueryDestination,omitempty" tf:"bigquery_destination,omitempty"`

	// If logging is enabled or not.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// Percentage of requests to be logged, expressed as a fraction in range(0,1]
	SamplingRate *float64 `json:"samplingRate,omitempty" tf:"sampling_rate,omitempty"`
}

type PredictRequestResponseLoggingConfigObservation struct {

	// BigQuery table for logging. If only given a project, a new dataset will be created with name logging_<endpoint-display-name>_<endpoint-id> where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name request_response_logging
	// Structure is documented below.
	BigqueryDestination *BigqueryDestinationObservation `json:"bigqueryDestination,omitempty" tf:"bigquery_destination,omitempty"`

	// If logging is enabled or not.
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// Percentage of requests to be logged, expressed as a fraction in range(0,1]
	SamplingRate *float64 `json:"samplingRate,omitempty" tf:"sampling_rate,omitempty"`
}

type PredictRequestResponseLoggingConfigParameters struct {

	// BigQuery table for logging. If only given a project, a new dataset will be created with name logging_<endpoint-display-name>_<endpoint-id> where will be made BigQuery-dataset-name compatible (e.g. most special characters will become underscores). If no table name is given, a new table will be created with name request_response_logging
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	BigqueryDestination *BigqueryDestinationParameters `json:"bigqueryDestination,omitempty" tf:"bigquery_destination,omitempty"`

	// If logging is enabled or not.
	// +kubebuilder:validation:Optional
	Enabled *bool `json:"enabled,omitempty" tf:"enabled,omitempty"`

	// Percentage of requests to be logged, expressed as a fraction in range(0,1]
	// +kubebuilder:validation:Optional
	SamplingRate *float64 `json:"samplingRate,omitempty" tf:"sampling_rate,omitempty"`
}

type PrivateEndpointsInitParameters struct {
}

type PrivateEndpointsObservation struct {

	// (Output)
	// Output only. Http(s) path to send explain requests.
	ExplainHTTPURI *string `json:"explainHttpUri,omitempty" tf:"explain_http_uri,omitempty"`

	// (Output)
	// Output only. Http(s) path to send health check requests.
	HealthHTTPURI *string `json:"healthHttpUri,omitempty" tf:"health_http_uri,omitempty"`

	// (Output)
	// Output only. Http(s) path to send prediction requests.
	PredictHTTPURI *string `json:"predictHttpUri,omitempty" tf:"predict_http_uri,omitempty"`

	// (Output)
	// Output only. The name of the service attachment resource. Populated if private service connect is enabled.
	ServiceAttachment *string `json:"serviceAttachment,omitempty" tf:"service_attachment,omitempty"`
}

type PrivateEndpointsParameters struct {
}

type PrivateServiceConnectConfigInitParameters struct {

	// Required. If true, expose the IndexEndpoint via private service connect.
	EnablePrivateServiceConnect *bool `json:"enablePrivateServiceConnect,omitempty" tf:"enable_private_service_connect,omitempty"`

	// If set to true, enable secure private service connect with IAM authorization. Otherwise, private service connect will be done without authorization. Note latency will be slightly increased if authorization is enabled.
	EnableSecurePrivateServiceConnect *bool `json:"enableSecurePrivateServiceConnect,omitempty" tf:"enable_secure_private_service_connect,omitempty"`

	// A list of Projects from which the forwarding rule will target the service attachment.
	ProjectAllowlist []*string `json:"projectAllowlist,omitempty" tf:"project_allowlist,omitempty"`
}

type PrivateServiceConnectConfigObservation struct {

	// Required. If true, expose the IndexEndpoint via private service connect.
	EnablePrivateServiceConnect *bool `json:"enablePrivateServiceConnect,omitempty" tf:"enable_private_service_connect,omitempty"`

	// If set to true, enable secure private service connect with IAM authorization. Otherwise, private service connect will be done without authorization. Note latency will be slightly increased if authorization is enabled.
	EnableSecurePrivateServiceConnect *bool `json:"enableSecurePrivateServiceConnect,omitempty" tf:"enable_secure_private_service_connect,omitempty"`

	// A list of Projects from which the forwarding rule will target the service attachment.
	ProjectAllowlist []*string `json:"projectAllowlist,omitempty" tf:"project_allowlist,omitempty"`
}

type PrivateServiceConnectConfigParameters struct {

	// Required. If true, expose the IndexEndpoint via private service connect.
	// +kubebuilder:validation:Optional
	EnablePrivateServiceConnect *bool `json:"enablePrivateServiceConnect" tf:"enable_private_service_connect,omitempty"`

	// If set to true, enable secure private service connect with IAM authorization. Otherwise, private service connect will be done without authorization. Note latency will be slightly increased if authorization is enabled.
	// +kubebuilder:validation:Optional
	EnableSecurePrivateServiceConnect *bool `json:"enableSecurePrivateServiceConnect,omitempty" tf:"enable_secure_private_service_connect,omitempty"`

	// A list of Projects from which the forwarding rule will target the service attachment.
	// +kubebuilder:validation:Optional
	ProjectAllowlist []*string `json:"projectAllowlist,omitempty" tf:"project_allowlist,omitempty"`
}

// EndpointSpec defines the desired state of Endpoint
type EndpointSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     EndpointParameters `json:"forProvider"`
	// THIS IS A BETA FIELD. It will be honored
	// unless the Management Policies feature flag is disabled.
	// InitProvider holds the same fields as ForProvider, with the exception
	// of Identifier and other resource reference fields. The fields that are
	// in InitProvider are merged into ForProvider when the resource is created.
	// The same fields are also added to the terraform ignore_changes hook, to
	// avoid updating them after creation. This is useful for fields that are
	// required on creation, but we do not desire to update them after creation,
	// for example because of an external controller is managing them, like an
	// autoscaler.
	InitProvider EndpointInitParameters `json:"initProvider,omitempty"`
}

// EndpointStatus defines the observed state of Endpoint.
type EndpointStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        EndpointObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:storageversion

// Endpoint is the Schema for the Endpoints API. Models are deployed into it, and afterwards Endpoint is called to obtain predictions and explanations.
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
type Endpoint struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	// +kubebuilder:validation:XValidation:rule="!('*' in self.managementPolicies || 'Create' in self.managementPolicies || 'Update' in self.managementPolicies) || has(self.forProvider.displayName) || (has(self.initProvider) && has(self.initProvider.displayName))",message="spec.forProvider.displayName is a required parameter"
	Spec   EndpointSpec   `json:"spec"`
	Status EndpointStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// EndpointList contains a list of Endpoints
type EndpointList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Endpoint `json:"items"`
}

// Repository type metadata.
var (
	Endpoint_Kind             = "Endpoint"
	Endpoint_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: Endpoint_Kind}.String()
	Endpoint_KindAPIVersion   = Endpoint_Kind + "." + CRDGroupVersion.String()
	Endpoint_GroupVersionKind = CRDGroupVersion.WithKind(Endpoint_Kind)
)

func init() {
	SchemeBuilder.Register(&Endpoint{}, &EndpointList{})
}
